---
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$$\text{EC 570} \\
\text{Fall 2021} \\
\text{Term Study Sheet} \\
\text{Drafted: November 27 2021}$$

This graduate level econometrics course is designed to teach students with basic quantitative and computer skills for econometric modeling with multivariate data with emphasis on economic policy evaluation. The specific topics of the course include detailed analysis and interpretation of univariate and multivariate regression, dummy variables, measurement issues and missing data issues. 

### Recommended Textbooks

• J. M. Wooldridge, Econometric Analysis of Cross Section and Panel Data, 2nd ed., The MIP Press, 2010. 

• W. H. Greene, Econometric Analysis, 7th ed., Pearson Education, 2012 

• A. Colin Cameron and Pravin K. Trivedi, Microeconometrics: Methods and Applications. Cambridge University Press, 2005. 

• J. M. Wooldridge: Introductory Econometrics (any edition)

### Econometrics overview

$\underline{\text{Econometrics}}$ - We need to use data and theory to answer causal questions i.e., to make statistical inferences and predictions.

### Ceteris Paribus

$\underline{\text{Ceteris Paribus}}$: how does variable $y$ change if variable $x$ is changed but $\underline{\text{all other relevant factors are held constant.}}$

### Steps to Econometrics Emperical Analysis

1. Form Question
2. Form Model
3. Formulate Hypothesis
4. Test Hypothesis
5. Interpret Results

$\underline{\text{Functional Form}}$ how things are related e.g. linear, exponential, quadratic, constant

$\underline{\text{Observational cross-sectional data is all observed at one time.}}$

$\underline{\text{Unobservable Factors}}$ we want to include unobservable factors in our model, use approximation?

Always remember **correlation $\ne$ cuausation**, and remember that there many types of relationships.

Let $x$ and $y$ be two random variables, suppose $y_i = \alpha + \beta x_i + u_i$ i.e., simple linear regression model.

Then, $\widehat{y_i} = \widehat{\alpha} + x_i \widehat{\beta}$ is *as close as possible* to $y_i ~\forall~ i$

So in simple linear regression, $x_i$ is a scalar random variable and $y_i$ is a scalar random variable.
While in multiple linear regression $x_i$ is a vector of random variables and $y_i$ is a scalar random variable. In addition, $y_i$ could be a vector of ranndom variables (seemingly unrelated regressions).

$$y_i: \text{outcome variable, regressand}\\
x_i: \text{regressor(s), covariates}$$

The $\underline{\text{ordinary least square estomator (OLS)}}$ is a value $\widehat{\alpha}, \widehat{\beta}$ that minimizes the sum of squared residuals: $SSR=\sum_{i=1}^{n}(y_i - (\widehat{\alpha} + x_i \widehat{\beta}))^2$

**Notations:**
$\overline{x} = \frac{\sum_{i=1}^{N} x_{i}}{N} ,~ \overline{y} = \frac{\sum_{i=1}^{N} y_{i}}{N} ,~ \overline{xy} = \frac{\sum_{i=1}^{N} x_{i}y_{i}}{N} ,~ \overline{x^2} = \frac{\sum_{i=1}^{N} x_{i}^2}{N} ,~ \overline{y^2} = \frac{\sum_{i=1}^{N} y_{i}^2}{N}$

### Assumptions, (?????#1-#4 are GM, #1-#5 are CLM???????)

#### 1. Linear in Parameters

#### 2. Random Sampling

#### 3. Sample variation in the explanatory variable

#### 4. Zero conditional mean

????????**Zero conditional mean assumption** is same as **mean independence assumption**????????

$E(u|x) = E(u) = 0$ The expected value of $u$ is unrelated to $x$ (and also the expected value of $u$ is zero).

#### 5. Homoskedasticity

The error has the same variance given any value of covariate, x: $Var(u|x) = \sigma^2 > 0$ where $\sigma^2$ is an (usually) unknown $\underline{\text{constant}}$.

Assumption #5 does not affect unbiasedness.

Together, assumptions #4 and #5 give us the conditional mean and variance of $y$, $E(y|x) = \beta_0 + \beta_1 x$ and $Var(y|x) = \sigma^2$. This means that the expected (average) value of $y$ does depend on $x$. However, (under assumption #5) the variance of $y$ does not depend on $x$.

#### 6. Normality

### Heteroskedasticity: Failure of assumption #5, variance of the error term $u$ (and hence the variance of $y$) does depend on the value of $x$.

### Lecture 5 Slide 13: Interpretaion of Quadratic simultenously as Linear term.

Lecture 5 Slide 12 shows graphical understanding of quadratic term. 

$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + u\\
\underline{\text{Interpretation of Quadratic Term}}\\
\beta_1 > 0 ~\&~ \beta_2 < 0, \text{ postitive but diminishing returns to } x\\
\beta_1 > 0 ~\&~ \beta_2 < 0, \text{ postitive and increasing returns to } x\\
\beta_1 > 0 ~\&~ \beta_2 = 0, \text{ constant returns to } x$$

Note that this model is still *linear in parameters* (an OLS assumption).

### Multiple Regression Model

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + u$$

Divide regression by $h(x)$ i.e., $*\frac{1}{h_(x)}$
$y^* = X^* \beta + U$

Each square residual is weighted by $\frac{1}{h_i}$.

# Some Review:

## Fundamentals (six subheadings)

### 1. Summation Operation

### 2. Proportion and Percentages

### 3. Random Variables

### 4. Expectations

$\underline{\text{Expected Value}}$ measures central tendency of probability distribution.

--> For random variable $X$ the expected value is $E[X]$ i.e., population mean of X.

Expected Value (Mean) is $E[X]=\int xf(x)$ i.e., expected value is each value scaled by that value's corresponding likelihood all added up.

### 5. Variances and Covariances

### 6. Condtitional Expectations

# UNORGANIZED BELOW THIS LINE -----------------------------------------------

Binary variable (dummy variable / indicator variable) has two categories,

$$X = \begin{cases} 1, & \mbox{if effect present} \\
2, & \mbox{if effect absent} \end{cases}$$

Interpretation of coefficient on dummy variable,

$$\beta = \frac{\partial E[y | X]}{\partial X}$$

### SOME LECTURE 2 THROUGH LECTURE 4 NOTES

$$\frac{d SSR}{d b} \biggr\rvert_{b = \hat{\beta}} = -2\sum_{i=1}^{N}\left( y_i - \hat\beta x_i\right)= 0 \implies \sum_{i=1}^{N}\hat{u_i}= 0\\$$

$$\implies \overline{x}*\overline{y} - \hat\beta (\overline{x})^2=0\\
\implies 
\left[\overline{x}*\overline{y} - \hat\beta (\overline{x})^2 \right] - \left[\overline{xy} - \hat\beta \overline{x^2}\right] = 0-0 \\
\overline{x}*\overline{y} - \hat\beta (\overline{x})^2 - \overline{xy} + \hat\beta \overline{x^2} = 0 \\ 
\hat\beta (\overline{x})^2 - \hat\beta \overline{x^2} = \overline{x}*\overline{y} - \overline{xy} \\
\hat\beta\left[ (\overline{x})^2 - \overline{x^2} \right] = \overline{x}*\overline{y} - \overline{xy}\\
\hat\beta = \frac{\overline{x}*\overline{y} - \overline{xy}}{\left[ (\overline{x})^2 - \overline{x^2} \right]}\\
\hat\beta = \frac{\overline{x}*\overline{y} - \overline{xy}}{ (\overline{x})^2 - \overline{x^2} }\\
\text{Notice, } \hat\beta = \frac{\text{Sample Covariance}(x_i,y_i)}{\text{Sample Variance}(x_i)} $$

$$\text{So the two F.O.C.s become}\\
~~~~~~~~~~~~~~~~~~~~~~ \overline{y} - \hat\beta \overline{x}= 0 ~~~~~~~~~~~ (1) \\
~~~~~~~~~~~~~~~~~~~~~~ \overline{xy} - \hat\beta \overline{x^2}= 0 ~~~~~~ (2)$$

$\text{Using these to derive OLS estimator (solve for } \hat{\beta}) \text{:}$

$$\text{Working from first F.O.C.,}\\
\sum_{i=1}^{N}\left( y_i - \hat\beta x_i\right)= 0\\
\sum_{i=1}^{N}y_i - \sum_{i=1}^{N}\hat\beta x_i= 0 \\
\sum_{i=1}^{N}y_i - \hat\beta\sum_{i=1}^{N} x_i= 0 \\
~~~~~~~~~~~~~~~~~~~~~~~~ N\overline{y} - \hat\beta N\overline{x}= 0  ~~~~~ (N >0 \& \in\mathbb{R})\\
\overline{y} - \hat\beta \overline{x}= 0$$

$$\text{"Regression with random variables"} \\
\text{"covariates and outcomes"} \\
\text{"K parameters, K unknowns, K slopes"} \\
\overline{x} = \frac{\sum_{i=1}^{n} x_{i}}{n} \\
\overline{y} = \frac{\sum_{i=1}^{n} y_{i}}{n} \\
\overline{xy} = \frac{\sum_{i=1}^{n} x_{i}y_{i}}{n} \\
\overline{x^2} = \frac{\sum_{i=1}^{n} x_{i}^2}{n} \\
\overline{y^2} = \frac{\sum_{i=1}^{n} y_{i}^2}{n} \\
\text{"Normal Equations"} \\
\hat{\beta} = \frac{\overline{xy} - \overline{x}
* \overline{y}}{\overline{x^2} - (\overline{x})^2} = \frac{Sample ~cov}{Sample ~var} \\
\hat{\alpha} = \overline{y} - \overline{x}\hat{\beta} \\
\text{Lecture 2 Slide 10} \\
\frac{1}{n} \sum_{i=1}^{n}u_i = 0 \\
\sum_{i=1}^{n} x_i \hat{u_i} = 0 \\
\overline{y} = \hat{\alpha} + \hat{\beta}x_i \implies (\overline{x},\overline{y}) \text{ on line}. \\
SST = \text{Total Sample Variation } y_i = \sum_{i=1}^{n}(y_i - \overline{y})^2 \\
SST = SSR+SSE \\
\text{Measure Linear relation w/ }R^2 \\
\text{RANK CONDITION IS THAT } (X^TX) \text{ IS INVERTIBLE}. \text{i.e., columns of } X \text{ are linearly independent.} \\
\text{"Goodness of fit" } R^2 \in [0,1], ~ R^2=\frac{SSE}{SST} \\
x_i = [1, x_{i2},\dots] \\
(X^TX)\hat{\beta}=X^TY \\
\text{Want } \hat{\beta} \\
(X^TX)^{-1} \rightarrow \text{ Rank Condtion must be satisfied.}\\
X_i \text{ Data matrix} \\
Y_i \text{ Prediction collumn vector} \\
\text{Projection Matrix: } P_X = X(X^TX)^{-1}X^T \\
P_X \text{ property: } \text{ Symmetric: } P_X^T = P_X \\
P_X \text{ property: } \text{ Idempotent: } P_XP_X = P_X \\
P_X \text{ property: } \mathbb{R}^n\rightarrow\mathbb{R}^{n-1}$$

$$\text{Vector of residuals } \hat{U} \\
\hat{U} = Y - \hat{Y} \\
\hat{U} = Y - X(X^TX)^{-1}X^TY \\
\hat{U} = (I_N - P_X)Y + M_XY\\
\text{Residual Matrix }M_X \\
M_X \text{ property: } \text{ Symmetric: } M_X^T = M_X \\
M_X \text{ property: } \text{ Idempotent: } M_XM_X = M_X \\
M_XX=0 \\
M_XP_X=0 \\
\text{Lecture 3}\\
E(u|x) = E(u) \rightarrow Cov(x,u)=E(xu)=0 \\
E(u) = 0 \\
y = \beta_0 + \beta_1 x + u \rightarrow u = y - (\beta_0 + \beta_1 x) \\
\text{So the two population conditions or moments are} \\
E(xu) = E[x(y-\beta_0 - \beta_1x)] = 0 \\
E(u) = E(y - \beta_0 - \beta_1x) = 0 \\
\text{The sameple counter parts are} \\
n^{-1}\sum_{i=1}^{n}x_i (y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 \\
n^{-1}\sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 \\
\text{Note } n^{-1}=\frac{1}{n} \\
\text{METHOD OF MOMENTS: use sample counterparts (of the population moments) to define estimators.} \\
\text{With these equations and a sample of data } (x_i\text{'s and }y_i\text{'s), we can calculate }\\
\hat{\beta_0} \text{ and } \hat{\beta_1}, \text{ our best guesses for the true values } \beta_0 \text{ and } \beta_1.\\
\text{Start with second moment:} \\
n^{-1}\sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 \\
\overline{y} - \hat{\beta_0} - \hat{\beta_1}\overline{x} = 0 \rightarrow \hat{\beta_0} = \overline{y} - \hat{\beta_1} \overline{x}$$ 

$$\text{So now we have} \\
1. ~ n^{-1}\sum_{i=1}^{n} x_i(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) \\
2. ~ \hat{\beta_0} = \overline{y} - \hat{\beta_1} \overline{x} \\
\text{Plug (2) into (1) to get} \\
3. ~ n^{-1}\sum_{i=1}^{n} (y_i - (\overline{y} - \hat{\beta_1} \overline{x}) - \hat{\beta_1}x_i) = 0 \\
\text{We can rearrange (3) to get} \\
\sum_{i=1}^{n} x_i(y_i - \overline{y}) = \hat{\beta_1} \sum_{i=1}^{n} x_i (x_i - \overline{x})$$

$$\text{Finally, we can rearrange this} \\
\sum_{i=1}^{n}x_i (y_i - \overline{y} = \hat{\beta_1} \sum_{i=1}^{n} x_i (x_i - \overline{x}) \\
\text{to get} \\
\hat{\beta_1} = \frac{\sum_{i=1}^{n}x_i (y_i - \overline{y})}{\sum_{i=1}^{n}x_i (x_i - \overline{x})} \\
\text{From summation properties,} \\
\sum_{i=1}^{n}x_i (y_i - \overline{y}) = \sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y}) \\
\sum_{i=1}^{n}x_i (y_i - \overline{x}) = \sum_{i=1}^{n}(x_i - \overline{x})^2 \\
\text{So go from} \\
\hat{\beta_1} = \frac{\sum_{i=1}^{n} x_i (y_i - \overline{y})}{\sum_{i=1}^{n} x_i (x_i - \overline{x})} \\
\text{to} \\
\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i-\overline{x}) (y_i - \overline{y})}{\sum_{i=1}^{n} (x_i - \overline{x})^2 } \\
\text{As long as} \\
\sum_{i-1}^{n} (x_i - \overline{x})^2 > 0 \\
\text{This gives us a formula for computing the slope estimate using the data in our sample} \\
\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i-\overline{x}) (y_i - \overline{y})}{\sum_{i=1}^{n} (x_i - \overline{x})^2 }  = \frac{Sample Covariance (x_i,y_i)}{Sample Variance of x_i}$$

$$\text{Lecture 4} \\
\text{Todays 3 topics are:} \\
1. \text{ Statistical properties of OLS estimators} \\
2. \text{ Unbiasedness} \\
3. \text{ Variance of the OLS estimators} \\
\text{REMEMBER THAT }x \text{ and } u \text{ ARE RANDOM VARIABLES.}\\
\text{THE STATISTICAL PROPERTIES OF THE OLS ESTIMATORS ARE}\\
\text{DERIVED CONDITIONAL ON THE VALUES OF THE COVARIATES IN OUR SAMPLE.} \\
\text{THIS IS IMPORTANT IN PROOFS.} \\
\text{UNBIASEDNESS says that over infinite samples if we averaged the estimates we'd get the true value.} \\
(E(\hat{\beta})=\beta) \\
E(\hat{\beta_1} | x)=\beta_1 \\
E(\hat{\beta_0} | x)=\beta_0 \\
\text{Assumptions for unbiasedness: } \\
\text{1. Linear in parameters} \\
\text{2. Random sampling} \\
\text{3. Sample variation in the explanatory variable7} \\
\text{4. Zero condtional mean} \\ 
\text{Linear parameters means each variable (whatever it is) is simple scaled by parameter} \\
\text{Random sampling means that we are picking random observations: NOT BASED UPON } y_i,~x_i,\text{ or } u_i. \\
\text{We need some variation in the things we are interested in studying} \\
\text{Zero conditional mean  means that in the population, } x \text{ does not give us any information about the value of } u. \\
E(u|x)=E(u)=0 \text{ THIS IS CRITICAL ASSUMPTION}$$

$$\text{er Assumptions #1-#4,} \\
E(\hat{\beta_1} | x) = \beta_1 ~\&~ E(\hat{\beta_0} | x) = \beta_0 \\
\text{If } x ~\&~ \text{are correlated our OLS estimators are likely to be biased.}\\
\text{The expected values are condtional on the sample values of the covariate.} \\
\text{Homoskedasticity, Heteroskedasticity, Estimating error variance} \\
\text{Besides unbiasedness, we are also interested in how we can expect our estimates to be scattered around the true value.} \\
\text{When picking an estimator we care about unbiasedness AND precision (small variance).} \\
\text{Assumption 5: Homoskedasticity} \\
\text{Homoskedasticity: The error has the same variance given any value of the explanatory variable, x:} \\
Var(u|x)=\sigma^2 > 0 \\
\text{where } \sigma^2 \text{ is an (usually) unknown constant.} \\
\text{The variance of } u \text{ is constant w.r.t. } x \text{, i.e., the variance of } u \text{ is unrelated to } x. \\
\text{Assumption 5 does not affect unbiasedness.} \\
\text{Together assumptions 4 adn 5 give condition mean and variance of y} \\
E(y|X) = E((\beta_0+\beta_1x+u)|x)=\beta_0 + \beta_1 x \\
Var(y|x) = \sigma^2 \\
\text{Assumption 5 means the variance of } y \text{ does not depend on } x.$$

$$\text{Heteroskedasticity is if assumption #5 is not satisified.} \\
\text{i.e., The variance of the error term } u \text{(and hance the variance of } y \text{) does depennd on the value of } x. \\
\text{SPECIFIC CONTEXT tells us whether homoskedasticity is satisified and whether if zero conditional mean is satisified.}$$

$$\text{Under Assumptions #1-#5 (all of them),} \\
Var(\hat{\beta_1})=\frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \overline{x})^2} = \frac{\sigma^2}{\text{Sample Var(x)}} \\
Var(\hat{\beta_0}) = \frac{\sigma^2\left( \frac{1}{n}\sum_{i=1}^{n}x_i^2\right)}{\sum_{i=1}^{n}(x_i - \overline{x})^2} \\
\text{As the error variance } (\sigma^2) \text{ goes up, so does the variance of } \hat{\beta_1}. \\
\text{Sample variance of } x \text{ decreases variance of } \hat{\beta_1}. \\
\text{The bigger the sample size, the smaller variance of } \hat{\beta_1}. \\
sd(\hat{beta_1})=\frac{\sigma}{\sqrt{\text{Sample Var(x)}}} \\
\text{Remember that } \sigma^2 = Var(u) = E(u^2) \\
\text{Unbiased estimator of } \sigma^2 \text{ is}\\
\frac{1}{n}\sum_{i=1}^{n}u_i^2 \\
\text{where } u_i = y_i - \beta_0 - \beta_1x_i \\
\hat{u_i} = y_i - \hat{\beta_0} - \hat{\beta_1}x_i\\
\hat{u_i}\ne u_i \\
\text{The unbiased estimatore of } \sigma^2 \text{ is} \\
\sigma^2 = \frac{1}{n-2}\sum_{i=1}^{n}\hat{u_i}^2=\frac{SSR}{n-2}=s^2\\
\text{where } E(\hat{\sigma}^2)=\sigma^2 \text{ and } SSR \text{ is the sum of squard residuals.}\\
\text{What we really need to calculate}\\
sd(\hat{\beta_1})=\frac{\sigma}{\sqrt{\text{Sample Var(x)}}}\\
\text{is } \sigma \text{ not } \sigma^2, \text{ so we use } \hat{\sigma} = \sqrt{\hat{\sigma}^2}, \\
\text{ which is also called the standard error of the regression (SER) or root mean squard error (RMSE).}$$

$$\text{Now we can calculate the sandard error of } \hat{\beta_1}, \text{ i.e., the accuracy of our estimate:} \\
se(\hat{\beta_1})=\frac{\hat{\sigma}}{\sqrt{\text{Sample Var(x)}}} = \frac{\sqrt{\frac{1}{n-2}\sum_{i=1}^{n}\hat{u}_i^2}}{\left(\sum_{i=1}^{n}(x_i)\right)^{\frac{1}{2}}} \\
\text{We call this the standard error (rahter than standard deviation) because we had to use an estimate of } \sigma.\\
\text{Usually, we report our estimate } se(\hat{\beta_1}) \text{ below oru estimate of } \hat{\beta_1} \text{ to be. This will be very helpful in hypothesis testing.} \\
\text{LECTURE 4 SUMMARY}\\
\text{SUMMARY: Homoskedasticity} \\
\text{Homooskedasticity: variance of the error term does not depend on the value of }x\\
Var(u|x)=\sigma^2 > 0 \\
\text{Heteroskedasticity: variance of the error term does depend on the value of }x\\
Var(u|x)=f(x) > 0 \\
\text{Homoskedasticity/Heteroskedasticity has NOTHING to do with whether or not OLS estimator is unbiased.} \\
\text{SUMMARY: Sampling Variance of OLS} \\
\text{Under assumptions #1-#4 AND #5 (homoskedasticity), formula for variance of } \hat{\beta_1} \text{ is} \\
Var(\hat{\beta_1}) = Var(\hat{\beta_1})=\frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \overline{x})^2} = \frac{\sigma^2}{\text{Sample Var(x)}} \\
Var(\hat{\beta_1}) \text{ is smaller when} \\
\bullet \text{As the error variance } (\sigma^2) \text{ goes down} \\
\bullet \text{As the sample variance of } x \text{ goes up} \\
\bullet \text{The bigger the sample size} \\
\text{SUMMARY: Estimating the error variance} \\
se(\hat{\beta_1})=\frac{\hat{\sigma}}{\sqrt{\text{Sample Var(x)}}} = \frac{\sqrt{\frac{1}{n-2}\sum_{i=1}^{n}\hat{u}_i^2}}{\left(\sum_{i=1}^{n}(x_i)\right)^{\frac{1}{2}}} \\
\text{Usually, we report our estimate of } se(\hat{\beta_1}) \text{ below our estimate of } \hat{\beta_1} \text{ so we can test hypotheses about } \beta_1.$$

### Consider Reichenbach's Principle

[Minutephysics fast and simple video about combing knowledge of causal relationships and observational knowledge of correlations to infer about additional causal relationships.](https://youtu.be/HUti6vGctQM)
Minutephysics's key takeaway is that correlation can imply causation if you analyze causal models.

Of course with experimental data, in which the variables are controlled and experimental units are randomly assigned and independent, correlation would imply causation.